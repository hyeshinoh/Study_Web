{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다룰 내용\n",
    "- Scrapy 개요\n",
    "- xpath\n",
    "    - xpath의 기본 문법\n",
    "    - scrapy shell 환경에서 xpath\n",
    "    - scrapy jupyter notebook xpath\n",
    "    - 네이버 실시간 검색어, 다음실시간 검색어, 지마켓 베스트 200\n",
    "- scrapy 프로젝트를 만들어서 크롤링\n",
    "    - 네이버 영화 사이트에서 현재 상영영화 링크를 크롤링\n",
    "    - 크롤링한 링크에서 영화 제목과 누적관객수 데이터를 크롤링\n",
    "    - csv 파일로 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scrapy\n",
    "- 웹사이트에서 데이터 추출을 하기 위한 오픈소스 프레임워크 (패키지가 아님)\n",
    "- http://scrapy.org\n",
    "- 설치\n",
    "    - windows: `conda install -c conda-forge scrapy`\n",
    "    - mac: `pip3 install scrapy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. xpath\n",
    "지금까지 배웠던 css selector가 아닌 xpath를 이용해서도 웹페이지의 html element를 선택할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 xpath의 기본 문법\n",
    "예시 xpath: 네이버 실시간 검색어 순위에서 [copy xpath]로 복사  \n",
    "`//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li[1]/a/span[2]`\n",
    "- `//`: 최상위 element를 의미\n",
    "- `*`: 조건에 맞는 하위 element를 모두 살펴봄 (css selector에서 하위 element 검색: 한칸 띄우기)\n",
    "- `[@id=\"PM_ID_ct\"]`: id가 PM_ID_ct 인 element를 선택\n",
    "    - `[@<key>=<value>]` 형태 (css selector와 다르게 모든 속성을 `@<key>`로 표현)\n",
    "- `/`: 바로 아래 element를 살펴봄 (css selector의 `>` 기호와 같은 의미)\n",
    "- `[number]`: number번째 element를 선택 (0번부터가 아니라 1번부터 시작함)\n",
    "- `.`: 현재 element를 의미\n",
    "- `not()`: 조건이 아닌 element를 찾음\n",
    "    - `not([@id=test})`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 네이버 실시간 검색어 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 스크래피 셸 사용\n",
    "- `$ scrapy shell \"<url>\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) jupyter notebook 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from scrapy.http import TextResponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 웹페이지에 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://naver.com\"\n",
    "rep = requests.get(url)\n",
    "response = TextResponse(rep.url, body=rep.text, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 네이버 실시간 검색어 1위 element 객체\n",
    "- xpath로 select하면 list로 return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li[1]/a/span[2]' data='<span class=\"ah_k\">우왁굳</span>'>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.xpath('//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li[1]/a/span[2]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 네이버 실시간 검색어 20개 element 객체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]' data='<span class=\"ah_k\">우왁굳</span>'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]' data='<span class=\"ah_k\">홍현희</span>'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]' data='<span class=\"ah_k\">김수현</span>'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]' data='<span class=\"ah_k\">오영택</span>'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]' data='<span class=\"ah_k\">미세먼지</span>'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]' data='<span class=\"ah_k\">주윤발</span>'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]' data='<span class=\"ah_k\">박지원</span>'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]' data='<span class=\"ah_k\">환희유치원</span>'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]' data='<span class=\"ah_k\">제이쓴</span>'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]' data='<span class=\"ah_k\">동탄 환희유치원</span>'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]' data='<span class=\"ah_k\">브리짓 존스의 베이비</span>'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]' data='<span class=\"ah_k\">고현정</span>'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]' data='<span class=\"ah_k\">추상미</span>'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]' data='<span class=\"ah_k\">조현민</span>'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]' data='<span class=\"ah_k\">장학영</span>'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]' data='<span class=\"ah_k\">강병규</span>'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]' data='<span class=\"ah_k\">이한샘</span>'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]' data='<span class=\"ah_k\">자이언티</span>'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]' data='<span class=\"ah_k\">오늘 미세먼지 농도</span>'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]' data='<span class=\"ah_k\">존조</span>'>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 네이버 실시간 검색어 20개 element 객체\n",
    "response.xpath('//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 객체의 text만 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]/text()' data='우왁굳'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]/text()' data='홍현희'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]/text()' data='김수현'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]/text()' data='오영택'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]/text()' data='미세먼지'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]/text()' data='주윤발'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]/text()' data='박지원'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]/text()' data='환희유치원'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]/text()' data='제이쓴'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]/text()' data='동탄 환희유치원'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]/text()' data='브리짓 존스의 베이비'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]/text()' data='고현정'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]/text()' data='추상미'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]/text()' data='조현민'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]/text()' data='장학영'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]/text()' data='강병규'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]/text()' data='이한샘'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]/text()' data='자이언티'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]/text()' data='오늘 미세먼지 농도'>,\n",
       " <Selector xpath='//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]/text()' data='존조'>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.xpath('//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]/text()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 문자열만 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['우왁굳', '홍현희', '김수현', '오영택', '미세먼지', '주윤발', '박지원', '환희유치원', '제이쓴', '동탄 환희유치원']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.xpath('//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li/a/span[2]/text()')[:10].extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `.`을 이용해서 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "우왁굳\n",
      "홍현희\n",
      "김수현\n",
      "오영택\n",
      "미세먼지\n",
      "주윤발\n",
      "박지원\n",
      "환희유치원\n",
      "제이쓴\n",
      "동탄 환희유치원\n",
      "브리짓 존스의 베이비\n",
      "고현정\n",
      "추상미\n",
      "조현민\n",
      "장학영\n",
      "강병규\n",
      "이한샘\n",
      "자이언티\n",
      "오늘 미세먼지 농도\n",
      "존조\n"
     ]
    }
   ],
   "source": [
    "keywords = response.xpath('//*[@id=\"PM_ID_ct\"]/div[1]/div[2]/div[2]/div[1]/div/ul/li')\n",
    "\n",
    "for keyword in keywords:\n",
    "    print(keyword.xpath('./a/span[2]/text()')[0].extract())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 다음 실시간 검색어 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 다음 실시간 검색어 리스트 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['박지원', '추상미', '우왁굳', '강예빈', '김수현 아나운서', '장학영', '환희유치원', '미세먼지', '존조', '서유정']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://daum.net\"\n",
    "rep = requests.get(url)\n",
    "response = TextResponse(rep.url, body=rep.text, encoding=\"utf-8\")\n",
    "response.xpath('//*[@id=\"mArticle\"]/div[2]/div[1]/div[2]/div[1]/ol/li/div/div[1]/span[2]/a/text()').extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 gmarket best item 200 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 웹페이지에 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://corners.gmarket.co.kr/Bestsellers\"\n",
    "rep = requests.get(url)\n",
    "response = TextResponse(rep.url, body=rep.text, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 베스트 200 아이템 제목 문자열 리스트로 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['일본수출 초극세사이불/백화점납품/ 피톤치드가공10mm',\n",
       " '제로스킨 아이폰 갤럭시 핸드폰케이스 6 7 8 X S 노트',\n",
       " '(국산)고급 반코팅 100켤레 이중코팅 면 목장갑 작업',\n",
       " '[다이소]공식판매처/택배박스/봉투/로고인쇄/당일발송/소량']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = response.xpath('//*[@id=\"gBestWrap\"]/div/div[3]/div[2]/ul/li/a/text()').extract()\n",
    "print(len(titles))\n",
    "titles[195:199]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 200개 아이템에서 li 클래스가 first인 데이터만 가져오기\n",
    "- 클래스가 first인 아이템은 한 줄에 4개씩 나타나는 상품 중 각 줄의 첫번째 상품"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = response.xpath('//*[@id=\"gBestWrap\"]/div/div[3]/div[2]/ul/li[@class=\"first\"]/a/text()').extract()\n",
    "len(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 200개 아이템에서 li 클래스가 first가 아닌 데이터만 가져오기\n",
    "- `not()`을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = response.xpath('//*[@id=\"gBestWrap\"]/div/div[3]/div[2]/ul/li[not(@class=\"first\")]/a/text()').extract()\n",
    "len(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scrapy project\n",
    "- jupyter notebook이 아닌 atom에서 작성\n",
    "- 네이버 영화 페이지에서 현재 상영되고 있는 영화의 제목과 관람객수를 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 scrapy 프로젝트 생성\n",
    "- project 생성: `$ scrapy startproject <name>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'crawler', using template directory '/usr/local/lib/python3.6/site-packages/scrapy/templates/project', created in:\n",
      "    /Users/hyeshinoh/Workspace/Study_Web/crawler\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd crawler\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "! scrapy startproject crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 scrapy 프로젝트 파일 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawler\n",
      "├── crawler\n",
      "│   ├── __init__.py\n",
      "│   ├── __pycache__\n",
      "│   ├── items.py\n",
      "│   ├── middlewares.py\n",
      "│   ├── pipelines.py\n",
      "│   ├── settings.py\n",
      "│   └── spiders\n",
      "│       ├── __init__.py\n",
      "│       └── __pycache__\n",
      "└── scrapy.cfg\n",
      "\n",
      "4 directories, 7 files\n"
     ]
    }
   ],
   "source": [
    "! tree crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### project 구조\n",
    "- spiders directory \n",
    "    - 어떤 웹사이트를 어떻게 크롤링할 것인가를 명시하고, 각각의 웹 페이지의 어떤 부분을 스크래핑할 것인지 명시하는 클래스가 모여있는 디렉토리\n",
    "    - 여러개의 spider.py 파일을 만들어 사용할 수 있음\n",
    "- items.py\n",
    "    - 크롤링하는 데이터에 대해 정의하는 클래스가 있는 파일(MVC → M)\n",
    "- pipelines.py\n",
    "    - item 객체 형태로 크롤링을 하고 출력하기 전에 item을 받아서 실행하는 파일이 정의되어 있는 파일\n",
    "    - item을 자유롭게 가공하거나 다양한 파일 형태로 저장할 수 있도록 하는 클래스\n",
    "- settings.py\n",
    "    - spider나 item pipeline 등이 어떻게 동작하게 할지에 대한 세부적 설정이 담겨 있는 파일 \n",
    "    - e.g. robots.txt 정책을 따를 것인지 안 따를 것인지, pipeline을 사용할지 안 할지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 크롤링 코드 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ※ yield"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### iterator와 generator\n",
    "- iterator: 순서가 있는 데이터의 집합\n",
    "- generator: 함수가 호출될 때마다 순서대로 결과가 나오는 집합\n",
    "    - `return`으로 한번에 데이터를 return하는 것보다 `yield`로 하나씩 return하면 resource를 절약할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator data\n",
    "ls = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator data\n",
    "def number():    # generator를 만드는 함수\n",
    "    yield 1\n",
    "    yield 2\n",
    "    yield 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object number at 0x11052c620>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = number()\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-8d5cb7b534a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n.__next__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) items.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://doc.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class CrawlerItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    title = scrapy.Field()\n",
    "    count = scrapy.Field()\n",
    "    pass\n",
    "\n",
    "#######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) spider.py\n",
    "- 가장 먼저 실행되는 파일\n",
    "- 기본적 변수\n",
    "    - name: scrapy 명령어로 spider를 실행할 때 argument로 쓰는 name\n",
    "    - domain: 크롤링할 domain\n",
    "    - start_urls: 가장 먼저 크롤링을 시작할 (처음에 request를 던질) url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 크롤링 함수 작성\n",
    "먼저, jupyter notebook 상에서 네이버 영화 페이지에서 현재 상영영화 링크 리스트 크롤링해본다.  \n",
    "(jupyter notebook 상에서 크롤링이 제대로 되는지 확인해보고, 파이썬 파일로 작성하기 위함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 상영중인 영화 리스트 받아오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://movie.naver.com/movie/running/current.nhn\"\n",
    "rep = requests.get(url)\n",
    "response = TextResponse(rep.url, body=rep.text, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://movie.naver.com/movie/bi/mi/basic.nhn?code=167105\n",
      "https://movie.naver.com/movie/bi/mi/basic.nhn?code=119428\n",
      "https://movie.naver.com/movie/bi/mi/basic.nhn?code=155356\n",
      "https://movie.naver.com/movie/bi/mi/basic.nhn?code=168050\n",
      "https://movie.naver.com/movie/bi/mi/basic.nhn?code=163533\n",
      "https://movie.naver.com/movie/bi/mi/basic.nhn?code=168023\n",
      "https://movie.naver.com/movie/bi/mi/basic.nhn?code=178402\n",
      "https://movie.naver.com/movie/bi/mi/basic.nhn?code=164992\n",
      "https://movie.naver.com/movie/bi/mi/basic.nhn?code=172040\n",
      "https://movie.naver.com/movie/bi/mi/basic.nhn?code=97612\n"
     ]
    }
   ],
   "source": [
    "links = response.xpath('//*[@id=\"content\"]/div[1]/div[1]/div[3]/ul/li/dl/dt/a/@href')[:10].extract()\n",
    "for link in links:\n",
    "    print(response.urljoin(link))    # domain과 연결시켜줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 영화의 제목과 누적 관객수 받아오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://movie.naver.com/movie/bi/mi/basic.nhn?code=159892\"\n",
    "rep = requests.get(url)\n",
    "response = TextResponse(rep.url, body=rep.text, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page_contents(self, response):\n",
    "    item = CrawlerItem()\n",
    "    item[\"title\"] = response.xpath('//*[@id=\"content\"]/div[1]/div[2]/div[1]/h3/a[1]/text()').extract()[0]\n",
    "    try:\n",
    "        item[\"count\"] = response.xpath('//*[@id=\"content\"]/div[1]/div[2]/div[1]/dl/dd[5]/div/p[2]/text()').extract()[0]\n",
    "    except:\n",
    "        item[\"count\"] = \"0명\"\n",
    "    yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nm_spider.py 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "\n",
    "import scrapy\n",
    "from crawler.items import CrawlerItem    # 위에서 작성한 items.py를 불러옴\n",
    "\n",
    "# start_urls -> parse -> parse_page_contents 순으로 호출\n",
    "class NaverMovieSpider(scrapy.Spider):    # scrapy.Spider 클래스를 상속 받음 (크롤링에 필요한 기능이 들어가 있음)\n",
    "    name = \"naver_movie\"   \n",
    "    allow_domain = [\"https://movie.naver.com\"]\n",
    "    start_urls = [\n",
    "        \"https://movie.naver.com/movie/running/current.nhn\"\n",
    "    ]    \n",
    "\n",
    "    # link 리스트를 가져옴 (start url을 request로 던지고 그 response를 parse 함수가 받음)\n",
    "    def parse(self, response):\n",
    "        links = response.xpath('//*[@id=\"content\"]/div[1]/div[1]/div[3]/ul/li/dl/dt/a/@href')[:10].extract()\n",
    "        for link in links:\n",
    "            link = response.urljoin(link)\n",
    "            yield scrapy.Request(link, callback=self.parse_page_contents)  # yield가 10개 생성됨\n",
    "\n",
    "    # 각페이지의 link로 접속하여 데이터를 가져옴\n",
    "    def parse_page_contents(self, response):\n",
    "        item = CrawlerItem()    # item obj.를 만들어 줌\n",
    "        item[\"title\"] = response.xpath('//*[@id=\"content\"]/div[1]/div[2]/div[1]/h3/a[1]/text()').extract()[0]\n",
    "        try:\n",
    "            item[\"count\"] = response.xpath('//*[@id=\"content\"]/div[1]/div[2]/div[1]/dl/dd[5]/div/p[2]/text()').extract()[0]\n",
    "        except:\n",
    "            item[\"count\"] = \"0명\"\n",
    "        yield item      # 10개 만들어진 generator마다 한 번 실행됨\n",
    "\n",
    "#######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) settings.py\n",
    "- 크롤링하려는 사이트의 robots.txt 정책에 따라 obey 여부를 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = False\n",
    "\n",
    "#######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) spider 실행\n",
    "- 실행 위치: 프로젝트 디렉토리에서 실행(item.py 파일이 있는 디렉토리)\n",
    "- 명령어\n",
    "    - `$ scrapy crawl naver_movie`\n",
    "    - csv 파일로 결과 저장 옵션: `$ scrapy crawl naver_movie -o movie.csv`\n",
    "        - 단, 이렇게 저장하면 column 순서를 지정할 수 없음 (pipeline으로 지정할 수 있음)\n",
    "- 비동기 thread로 처리되기 때문에 영화 데이터가 순서대로 크롤링되지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2,833,821명</td>\n",
       "      <td>암수살인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15,248명</td>\n",
       "      <td>리즈와 파랑새</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>395,867명</td>\n",
       "      <td>곰돌이 푸 다시 만나 행복해</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0명</td>\n",
       "      <td>다이노 어드벤처2: 육해공 공룡 대백과</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>153,398명</td>\n",
       "      <td>스타 이즈 본</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18,318명</td>\n",
       "      <td>극장판 가면라이더 이그제이드: 트루 엔딩</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>193,789명</td>\n",
       "      <td>미쓰백</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>794,815명</td>\n",
       "      <td>그랜드 부다페스트 호텔</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5,352,401명</td>\n",
       "      <td>안시성</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3,249,358명</td>\n",
       "      <td>베놈</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count                   title\n",
       "0  2,833,821명                    암수살인\n",
       "1     15,248명                 리즈와 파랑새\n",
       "2    395,867명         곰돌이 푸 다시 만나 행복해\n",
       "3          0명   다이노 어드벤처2: 육해공 공룡 대백과\n",
       "4    153,398명                 스타 이즈 본\n",
       "5     18,318명  극장판 가면라이더 이그제이드: 트루 엔딩\n",
       "6    193,789명                     미쓰백\n",
       "7    794,815명            그랜드 부다페스트 호텔\n",
       "8  5,352,401명                     안시성\n",
       "9  3,249,358명                      베놈"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"movie.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pipeline을 사용해서 크롤링한 데이터를 column 순서를 지정해서 csv 파일로 저장할 수 있음\n",
    "- 크롤러를 실행할 때 CrawlerPipeline 객체를 생성하고 아이템을 하나씩 크롤링할 때마다 process_item 함수를 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "\n",
    "import csv\n",
    "\n",
    "class CrawlerPipeline(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.csvwriter = csv.writer(open(\"NaverMovie.csv\",\"wt\"))\n",
    "        self.csvwriter.writerow([\"title\", \"count\"])\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        row = []\n",
    "        row.append(item[\"title\"])\n",
    "        row.append(item[\"count\"])\n",
    "        self.csvwriter.writerow(row)\n",
    "        return item\n",
    "\n",
    "#######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "settings.py 수정 (주석 해제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "\n",
    "ITEM_PIPELINES = {\n",
    "    'crawler.pipelines.CrawlerPipeline':300,\n",
    "}\n",
    "\n",
    "#######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다시 scrapy 실행시 결과 파일의 column이 title, count 순서로 만들어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>암수살인</td>\n",
       "      <td>2,833,821명</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>리즈와 파랑새</td>\n",
       "      <td>15,248명</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>다이노 어드벤처2: 육해공 공룡 대백과</td>\n",
       "      <td>0명</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>곰돌이 푸 다시 만나 행복해</td>\n",
       "      <td>395,867명</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>스타 이즈 본</td>\n",
       "      <td>153,398명</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>극장판 가면라이더 이그제이드: 트루 엔딩</td>\n",
       "      <td>18,318명</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>안시성</td>\n",
       "      <td>5,352,401명</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>미쓰백</td>\n",
       "      <td>193,789명</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>그랜드 부다페스트 호텔</td>\n",
       "      <td>794,815명</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>베놈</td>\n",
       "      <td>3,249,358명</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    title       count\n",
       "0                    암수살인  2,833,821명\n",
       "1                 리즈와 파랑새     15,248명\n",
       "2   다이노 어드벤처2: 육해공 공룡 대백과          0명\n",
       "3         곰돌이 푸 다시 만나 행복해    395,867명\n",
       "4                 스타 이즈 본    153,398명\n",
       "5  극장판 가면라이더 이그제이드: 트루 엔딩     18,318명\n",
       "6                     안시성  5,352,401명\n",
       "7                     미쓰백    193,789명\n",
       "8            그랜드 부다페스트 호텔    794,815명\n",
       "9                      베놈  3,249,358명"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"NaverMovie.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 덧\n",
    "- yield, callback 구조를 이해하면 단계를 다양하게 구성할 수 있음\n",
    "- 동적페이지를 크롤링하려면 `__init__()` 함수로 driver를 만들고 driver를 사용해서 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고자료\n",
    "- 패스트캠퍼스, ⟪데이터사이언스스쿨 8기⟫ 수업자료"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
